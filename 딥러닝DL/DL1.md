# [딥린이 이윤정 기자 즐딥 도전 ①] 새 친구, 딥러닝을 소개합니다

> 인공지능(AI)을 세상을 널리 알린 건, 알파고였다. 이 알파고가 딥러닝을 한, 인공지능 모델. 
> 딥러닝을 배워보려는 '딥린이'를 위한 즐딥(즐겁게 딥러닝) 좌충우돌기를 20회에 걸쳐 연재한다. 

딥러닝이 딥(Deep)하기 보다, 핫(Hot)합니다. 글로벌 4대 공룡 IT기업 'MAGA(마이크로소프트·아마존·아마존·구글·마이크로소프트)' 모두 집중하는 분야. 투자가 몰리고 주가도 날아가네요. 

딥린이(딥러닝을 배우는 어린이 단계)에겐 활성화 함수 5가지를 넘어서야 그 개념이 잡히기 시작할거에요. 
이번 시리즈를 통해서 소개할 함수들은 시그모이드 함수, tanh 함수, ReLU 함수, ReLU 가족 함수, 소프트맥스 함수입니다. 그럼 시작해볼까요~!.

```
딥(deep) = 깊다     
러닝(learning) = 학습
깊은 학습..?
```

딥러닝 직역인 '깊은 학습'에서 무언가 깊이 파고든다는 느낌을 받을 수 있습니다. 
그렇지 않은가요?

우리는 각자 자기만의 공부 방식으로 공부합니다. 
▲학원에 다니거나 ▲과외를 하거나 ▲독학하거나 ▲동영상 강의를 시청하거나 하는 등. 
공부 하는데 여러 가지 길이 있는 것처럼 딥러닝은 컴퓨터가 공부하는 방법 중 하나라고 보면 됩니다~! 
사람만 공부하는 게 아니다. 컴퓨터도 공부합니다...  

컴퓨터 학습에서 딥러닝은 어마어마하게 많은 양의 데이터에서 패턴을 찾아내는 과정인데요. 
패턴을 찾아서 뭘 하냐고요? 사람이 기계에 일일이 입력하지 않아도 기계가 알아서 사람처럼 스스로 판단하는 기술을 가르칩니다. 

우선 사람의 학습은 어떻게 이루어지는지 생각해볼까요? 우리 뇌에는 뉴런이라는 신경 세포가 서로 신호를 주고받으면서 정보를 전달합니다. 
이 과정을 통해 사람은 학습을 하게 되는데, 기계는 인공 뉴런인 퍼셉트론으로 비슷하게 학습이 이루어집니다. 
그러니까 인간의 신경세포 = 기계의 퍼셉트론이라고 이해하면 됩니다!

여기까지 이해가 됐다면 이번 시리즈 주제인 활성화 함수에 대해 조금 더 배우기 쉬울거에요.  

자, 다시 우리 뇌로 돌아가서, 뇌에 있는 뉴런을 한번 봅시다. 뉴런의 구조를 보면 머리, 몸통, 꼬리로 구분할 수 있습니다. 
뉴런은 머리에서 전기 신호를 전달받아 몸통을 타고 꼬리 쪽으로 보냅니다. 
이때 꼬리에 도착한 전기 신호가 일정 크기 이상일 때만 이웃 뉴런으로 넘어가고 작으면 안 넘어갑니다!

인공 뉴런인 퍼셉트론도 비슷하게 작동하는데, 입력 신호가 그대로 출력되는 게 아니라 계산 과정을 거쳐서 출력되지요. 
이 계산 과정에 쓰이는 공식이 바로 활성화 함수입니다. 수학적 정의를 먼저 보면, 
활성화 함수는 입력과 가중치의 곱들의 합에 편향을 더한 값(a)을 받아 출력신호로 변환하는 함수. 
외계어는 일단 아닌데, 쉽게 이해가 되는 설명은 아니네요. 

```
입력 신호 a, 가중치 w
총합 x, x = a1*w1 + a2*w2
활성화 함수 f(), f(x) = y 
출력 신호 y
```

간단하게 식으로 표현해봤습니다. 조금 감이 잡히시나요? 
퍼셉트론의 입력 신호는 뉴런의 전기 신호와 같다고 생각하면 되요. 
이때 각 입력 신호마다 중요치를 매기게 되는데 이 중요치를 가중치 w라고 기억하면 됩니다. 

n개의 입력 신호가 있을 때, 입력 신호 a1 곱하기 가중치 w1 = x1이라고 해보죠. 
x1에서 xn까지 값의 합이 x이고, x가 바로 활성화 함수 f(x)의 입력값 x입니다. 
x를 f(x)에 대입했을 때 나오는 y는 출력값이라고 합니다. 

그래서 활성화 함수는 왜 쓰는 건데? 라고 물어보시면 바로 이 y를 얻기 위해서라고 할 수 있어요. 
앞 부분에 우리 뇌의 뉴런이 전기 신호를 전달하는 방법에 관해 설명했는데요. 
해당 뉴런에 전달된 신호들의 총합이 기준치 이상이어야 이웃 뉴런에 신호를 전달됩니다. 
딥러닝 네트워크에서 신호 흐름을 파악하여 각 퍼셉트론에 제한을 걸어둔 것이 활성화 함수입니다. 
결국 활성화 함수에서 적용한 공식의 기준을 만족하는 경우에만 신호가 이어갈 수 있는 것입니다. 

간단한 예제를 소개해볼게요. 

위 그래프를 보면 x = 0을 기준으로 x가 0보다 작으면 0을, 0보다 크면 1을 출력하는데요. 
이 활성화 함수의 기준치는 0 입니다. 
퍼셉트론에 계단 함수가 쓰였을 때 1이 출력되면 '활성'되고 0이 출력되면 '비활성' 된다. 생각보다 간단하지용?

지금까지 기본적으로 활성화 함수의 정체와 필요성에 대해 살펴보았습니다. 
다음 글은 시그모이드 활성화 함수에 대해 소개드리겠습니다~!
